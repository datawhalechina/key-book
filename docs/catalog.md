# 序言

*编辑：詹好，赵志民，王茂霖*

---

## 机器学习理论的成果与书籍

近年来，机器学习领域发展迅猛，相关的课程与教材层出不穷。国内的经典教材如周志华的 [《机器学习》](https://book.douban.com/subject/26708119) 和李航的 [《统计学习方法》](https://book.douban.com/subject/33437381)，为许多学子提供了机器学习的入门指引。而在国外，Tom Mitchell 的 *Machine Learning*、Richard O. Duda 等人的 *Pattern Classification*、Ethem Alpaydın 的 *Introduction to Machine Learning* 等书籍则提供了更为系统的学习路径。对于希望深入学习的读者，Christopher M. Bishop 的 *Pattern Recognition and Machine Learning*、Kevin P. Murphy 的 *Machine Learning - A Probabilistic Perspective*、Trevor Hastie 等人的 *The Elements of Statistical Learning* 等著作也能提供详尽的理论指导。这些书籍无论在国内外，都成为了学习机器学习的重要资源。

然而，从**机器学习理论**的角度来看，现有的学习材料仍存在不足之处。相比于聚焦机器学习算法的著作，专注于机器学习理论的书籍未得到足够的重视。尽管上述一些经典著作中涉及到理论探讨，但篇幅有限，往往仅以独立章节或片段呈现，难以满足深入研究的需求。

以往的机器学习理论经典教材大多为英文撰写。上世纪末围绕统计学习理论展开的讨论，催生了诸如 Vladimir Vapnik 的 *The Nature of Statistical Learning Theory* 和 *Statistical Learning Theory*，以及 Luc Devroye 等人的 *A Probabilistic Theory of Pattern Recognition* 等经典文献。近年来，Shai Shalev-Shwartz 和 Shalev Ben-David 的 *Understanding Machine Learning*，以及 Mehryar Mohri 等人的 *Foundations of Machine Learning* 进一步推进了这一领域的发展。虽然部分经典著作已有高质量的中文译本，但由中文作者撰写的机器学习理论入门书籍仍显不足。

如今，周志华、王魏、高尉、张利军等老师合著的 [《机器学习理论导引》](https://book.douban.com/subject/35074844)（以下简称《导引》）填补了这一空白。该书以通俗易懂的语言，为有志于学习和研究机器学习理论的读者提供了良好的入门指引。全书涵盖了 **可学性、假设空间复杂度、泛化界、稳定性、一致性、收敛率、遗憾界** 七个重要的概念和理论工具。

尽管学习机器学习理论可能不像学习算法那样能够立即应用，但只要持之以恒，深入探究，必将能够领悟到机器学习中的重要思想，并体会其中的深邃奥妙。

-- *詹好*

## 机器学习理论的应用与反思

近年来，机器学习的研究与应用取得了显著进展，经验性能，尤其是最先进技术（State of the Art, SOTA）指标，逐渐成为评价算法优劣的主要标准。在这一评价体系下，模型是否在基准数据集上取得领先结果，往往比其适用范围、失败模式或理论依据更受关注。然而，随着模型规模的不断扩大和应用场景的日益复杂，一些根本性问题开始浮现：为何模型能力的提升往往依赖规模堆叠而难以预测？为何在分布轻微偏移时，行为会出现显著失稳？又为何诸如幻觉、模式坍塌等现象在不同模型与任务中反复出现，却难以通过简单的工程修补彻底消除？

在计算机科学的历史中，类似的困惑并非首次出现。二十世纪三十年代，人们已经能够设计出在工程上极为复杂的计算装置，但“哪些问题原则上可以通过计算解决”仍然是一个模糊的问题。图灵通过形式化“算法”的概念，引入图灵机模型，从而给出了“可计算性”的严格定义。一个系统若具有图灵完备性，意味着它在理论上可以实现任何可计算的函数；而不可计算性结果则明确指出，存在一类问题，无论计算资源如何增长，都不可能通过算法解决。重要的是，这一边界并未限制计算机科学的发展，反而使研究者能够区分“工程尚未做到”与“原则上做不到”，从而避免在不可能的问题上消耗想象力。

机器学习的问题正好位于这一传统框架之外。一个函数是否可计算，并不意味着它能够从数据中被学习。学习并非直接访问目标函数，而是依赖有限样本、噪声观测以及特定的数据生成机制。正是在这一意义上，可计算性与可学习性出现了根本分离。学习理论中的许多不可能性结果，刻画的并不是现实世界的最坏情况，而是指出：如果我们拒绝对目标函数或数据分布施加任何结构性假设，那么学习这一任务在逻辑上便是不良定义的。

这一点在没有免费午餐定理（No Free Lunch, NFL）中体现得尤为清晰。该定理表明，在对问题分布不作任何假设的前提下，所有学习算法在平均意义上表现等价；不存在一种能够在所有问题上始终优于其他算法的通用方法。这一结论并非否认某些算法在实践中的成功，而是强调：任何成功都必然依赖于问题分布所隐含的结构。忽视这一前提，盲目追逐某一算法在特定时间点或特定任务上的优势，往往会在问题环境发生变化时付出代价。

在更具体的层面上，统计学习理论通过形式化的复杂度刻画，将这一“结构性前提”进一步加以明确。以 Vapnik–Chervonenkis（VC）维为代表的一类结果表明：若假设空间在组合意义上具有受控的复杂度，则可以在分布无关的条件下获得有限样本下的泛化保证；反之，若模型能够对任意有限样本实现任意标注，则在最坏情况下不存在统一的学习保证。需要强调的是，这类结论并非对现实问题的否定，而是清晰地划定了一条理论边界——它们刻画的是在拒绝任何分布或生成机制假设时，学习任务所面临的内在不确定性。正是这种“最坏情况”视角，促使后续研究不断引入更细致的结构假设，以缩小理论分析与经验成功之间的差距。

神经网络的发展历史为上述区分提供了重要例证。1987 年，Robert Hecht-Nielsen 证明，具有单隐层结构的前馈神经网络可以逼近任意连续的多元函数。这一结果后来被称为通用逼近定理（Universal Approximation Theorem, UAT），其核心含义在于：在表示能力层面，神经网络并不受限于简单函数族。随后，Vugar Ismailov 等人的工作进一步表明，即便是某些不连续函数，也可以通过类似结构进行逼近。这些结果在理论上极大拓展了神经网络的表达能力边界，但它们回答的是“是否存在某种参数配置”，而非“是否能够通过有限数据和可行的学习过程找到该配置”。通用逼近定理因此既是深度学习成功的重要思想源头，也是误解最为频繁的理论结果之一。

近年来，Transformer 模型的兴起再次凸显了理论分析的重要性。一方面，Transformer 在自然语言处理、视觉和多模态任务中展现出强大的经验性能；另一方面，近期的一系列理论工作开始系统解释其潜在能力。例如，有研究通过严格的数学构造证明，Transformer 可以模拟任意多项式规模的数字电路，从而在计算能力上具有极高的上界；也有工作表明，在适当的假设下，Transformer 架构构成了一类具有置换等变性的序列到序列函数的通用逼近器。这些结果并非宣称模型“无所不能”，而是表明：当任务结构与模型的归纳偏置高度匹配时，其能力可以被系统性地释放。

与此同时，关于语言生成的理论研究也揭示了模型能力背后的深层约束。在“生成而非预测”的设定下，研究者发现有效性、多样性与覆盖性之间存在不可避免的权衡关系。幻觉或模式坍塌并非简单的工程缺陷，而是生成问题在信息约束和可学习性条件下的结构性代价。这些分析为当前大模型的经验行为提供了重要的理论背景，也提醒我们，模型的成功与失败往往源于同一组假设。

由此可见，机器学习理论的意义并不在于神话模型的潜力，也不在于因为不可能性结果而放弃想象。相反，它通过揭示能力与限制所依赖的结构条件，使我们能够更精确地理解哪些问题在现有框架下是可行的，哪些则需要新的假设与工具。本书正是在这一立场下编写的。我们希望通过系统呈现机器学习理论中的关键概念、经典结果及其思想背景，帮助读者在面对模型的成功与失败时，区分结构性必然与工程偶然，从而在实践与研究中保持更清醒、也更具想象力的判断。

-- *赵志民*

## 机器学习理论的讲解与笔记

《导引》的讲解笔记在团队内部被亲切地称为《钥匙书》。“钥匙”寓意着帮助读者开启知识之门，解答学习中的疑惑。

《导引》作为一本理论性较强的著作，涵盖了大量数学定理和证明。尽管作者团队已尽力降低学习难度，但由于机器学习理论本身的复杂性，读者仍需具备较高的数学基础。这可能导致部分读者在学习过程中感到困惑，影响学习效果。此外，由于篇幅限制，书中对某些概念和理论的实例说明不足，也增加了理解的难度。

基于以上原因，我们决定编辑这本《钥匙书》作为参考笔记，对《导引》进行深入的注解和补充。其目的是帮助读者更快理解并掌握书中内容，同时记录我们在学习过程中的思考和心得。

《钥匙书》主要包含以下四个部分：

1. **概念解释**：介绍书中涉及但未详细阐释的相关概念。
2. **证明补充**：详细解释部分证明的思路，并补充书中省略的证明过程。
3. **案例分享**：增加相关实例，帮助读者加深对抽象概念的理解。

鉴于《导引》第一章的内容简明易懂，《钥匙书》从第二章开始详细展开。

对我个人而言，《机器学习理论导引》与*Understanding Machine Learning*和*Foundations of Machine Learning*一样，都是既“无用”又“有用”的书籍。“无用”在于目前的经典机器学习理论尚难全面解释深度学习，尤其是现代生成式大模型的惊人表现。然而，我坚信未来的理论突破将基于现有研究成果，开创新的篇章。因此，分析结论可能并非最重要，真正宝贵的是其中蕴含的思想和分析思路。数学作为一种强有力的工具，能够帮助我们更深入地理解和探索。我期望未来的深度学习能够拥有更多坚实的理论支撑，从而更好地指导实践。正如费曼所言：“What I cannot create, I do not understand.”——“凡我不能创造，我就不能理解。”希望大家能从这些理论中获得启发，创造出更有意义的成果。

另一方面，这本书也让我认识到自身的不足。不同于传统的机器学习算法教材，本书要求读者具备良好的数学功底，通过数学工具从更抽象的角度分析机器学习算法的性质，而非算法本身。学习之路或许漫长，但正如《牧羊少年的奇幻漂流》中所言：“每个人的寻梦过程都是以‘新手的运气’为开端，又总是以‘对远征者的考验’收尾。”希望大家能坚持经历考验，最终实现自己的梦想。

自《钥匙书》v1.0 版本发布以来，受到了众多学习者的关注。我们也收到了许多关于教材内容的疑问。为进一步深入理解相关知识，并记录团队对机器学习理论相关书籍的学习过程，我们将持续对《钥匙书》进行不定期更新，期待大家的关注。

-- *王茂霖*

## 项目成员贡献与特别鸣谢

[詹好](https://github.com/zhanhao93)负责了项目的初期规划与统筹，并参与了第一版的编辑和审核；[赵志民](https://github.com/zhimin-z)主导了项目二期的更新与维护，并负责全书最终编辑和校验；[李一飞](https://github.com/leafy-lee)参与了第1-5章内容的编辑；[王茂霖](https://github.com/mlw67)参与了第2-6章内容的编辑。

另外，特别鸣谢了[谢文睿](https://github.com/Sm1les)和[杨昱文](https://github.com/youngfish42)，他们共同提供了本书的在线阅读功能；[张雨](https://github.com/Drizzle-Zhang)对第2章的早期内容进行了修订，各成员的协作确保了本书高质量的编写和顺利完成。
