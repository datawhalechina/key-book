# 第3章：复杂性

*Edit: 王茂霖，李一飞，Hao ZHAN*

*Update: 06/12/2020*

---

类似于计算理论中的算法复杂度，机器学习理论也又对假设空间复杂度的研究。

假设空间复杂度$\mathcal H$与PAC科学性密切相光。假设空间$\mathcal H$越复杂，从中找到目标概念的难度越大。对于有限的假设空间，我们可以用包含的假设数目来刻画假设空间的复杂度。对于无限的假设空间，我们需要借助新的方法来描述复杂度。

## 1. 【概念补充】VC维

VC维是对与数据分布无关的假设空间$\mathcal H$的复杂程度的描述。假设空间$\mathcal H$的VC维是能被假设空间打散（shatter）的最大样本集大小：
$$
VC(\mathcal H)=max\{m:\Pi_{\mathcal H}(m)=2^m\}
$$
我们可以把VC维理解为假设空间$\mathcal H$在二元分类中的有效自由度，即当前假设空间能够进行多少种对分的能力。要证明一个假设空间的VC维为d，需要证明两点：

1. 存在大小为d的样本集D能被$\mathcal H$打散；
2. 任意大小为d+1的样本集D'都不能被$\mathcal H$打散。

我们的目标是是使经验误差尽可能地小，或者尽可能地接近泛化误差。于是我们可以通过假设空间的VC维来选择更好的模型。如果VC维很小，那么假设空间的表达能力受到了限制。此时经验误差可能就没有办法做到很小。但是也导致了模型发生预测偏差很大的坏事情可能性降低了许多。反之，如果VC维很大，那么假设空间的表达能力会很强。这意味着通常情况下，我们的经验误差可以训练地非常小。但随之而来的是经验误差与泛化误差区别很大的坏事情发生的可能性就变得很大。

对于VC维为无穷的假设空间$\mathcal H$，任意大小的样本集D都能被$\mathcal H$打散，此时该假设空间的增长函数$\Pi_{\mathcal H(m)}=2^m$。当VC维有限为d且$m\geqslant d$时，增长函数随着数据集大小呈多项式级增长。对于多分类问题，我们需要引入Natarajan维。



## 2. 【概念补充】Natarajan维

对于多分类问题，假设空间$\mathcal H$的Natarajan维是能被$\mathcal H$打散的最大样本集的大小。特别地，当类别数$K=2$时：
$$
VC(\mathcal H)=Natarajan(\mathcal H)
$$
Natarajan维的增长函数上界为：
$$
\Pi_{\mathcal H}(m)\leqslant m^dK^{2d}
$$
一般情况下，Natarajan维的复杂度随着样本数m和分类数K呈指数级增长。



## 3. 【概念补充】Rademacher复杂度

前面两种复杂性描述均未考虑数据自身的分布，这些界只是针对所有感兴趣的一些假设笼而统之的分析。在实际中，同一模型在不同的数据上的表型是不同的。此时样本的表示方法、几何结构、信噪比等性质都会对分类性能产生很大影响。这里引入Rademacher复杂度以刻画数据分布相关的假设空间复杂度，从而推导出更紧的泛化误差界。

函数空间$\mathcal F$关于$\mathcal Z$在分布$\mathcal D$上的Rademacher复杂度为：
$$
\Re_{\mathcal Z}(\mathcal F)=E_{Z\subset\mathcal Z:|Z|=m}[\hat\Re_{\mathcal Z}(\mathcal F)]=E_{Z\subset\mathcal Z:|Z|=m}[E_{\sigma}[\underset{f\in\mathcal F}{sup}\frac{1}{m}\sideset{}{_{i=1}^m}\sum\sigma_i f(z_i)]]
$$
这里$\sigma_i$是${-1,+1}$上服从均匀分布的随机变量。如果将$\sigma_i$改为其他分布，会得到一些别的复杂度定义。

值得一提的是，假设空间$\mathcal H$的Rademacher的复杂度上界以$O(\sqrt{\frac{ln\Pi_{\mathcal H}(m)}{m}})$的速度增长：
$$
\Re_m(\mathcal H)\leqslant\sqrt{\frac{2ln\Pi_{\mathcal H}(m)}{m}}
$$



## 4. 【概念补充】shattering 概念的可视化

**P40**中

提到了 shattering 的概念。即，若假设空间能够实现样本集上所有对分，则称样本集能够被该假设空间**打散（shattering）**。

这里用 $R^2$ 空间的示例进一步介绍打散的概念，以二维空间 $R^2$ 来说，一条任意的直线 $w_1x_1+w_2x_2+b=0$ 对于任意的三个点实现的对分（二分类）的所有情形如下：

<center><img src="./chapter3/img/shattering.png" width= "600"/></center>



可以发现，使用二维平面 $R^2$ 中的一条直线 $sign(wx+b)$ 可以实现三点的所有对分。

同样，考察四个点的情况，发现直线 $sign(wx+b)$ 并不能够实现对任意四点的对分。

例如，异或（XOR）问题：

<center><img src="./chapter3/img/xor.png" width= "300"/></center>



因此，根据VC维的定义， $sign(wx+b)$ 这样一个 $R^2$ 中的非齐次超平面的VC维为3。而对于齐次超平面 $sign(wx)$ ，由于其必须过原点，所以只能满足对任意两点的对分，因此其VC维为2。



## 5.【证明补充】定理3.5和定理3.6补充

**P50-P51**的**定理3.5**和**定理3.6**主要对线性超平面的VC维进行分析。

假设空间的VC维是能被 $h$ 打散的最大样本集的大小，如果其VC维为 $d$, 那么意味着：

- 存在一个大小d的样本集能被 $h$ 打散。因此在进行VC维的证明工作时，需要构造一个大小为d的样本集，证明在该样本集能够被打散。*Tips: 这也说明如果一个假设空间的VC维为d,其并不代表能打散所有大小为的d的样本集.*

- 对于任意的d+1维样本不能被其打散。一般用反证法进行证明。

基于此，就可以展开对 $R^d$ 中的齐次的线性超平面的考察。

**定理3.5**的证明，是通过构造 $R^d$ 中的 $d$ 个的单位向量组成了一个大小为d的样本集 $D=\{e_1,...,e_d\}$，然后利用超平面的可打散性证明了这个样本集的VC维至少是 $d$。然后，利用反证法说明对于任意 $R^d$ 中的 $d+1$ 个样本集是不能打散的。最终证明 $R^d$ 中的齐次的线性超平面的VC维为 $d$。

对于非齐次的利用升维转化为齐次可以利用齐次的结论同证。



## 6.【证明补充】定理3.8补充

**P52**定理3.8给出一个和空间维数无关的VC维刻画，这里对其不等式的思想和意义进行介绍。

**定理3.8**对于超平面族的刻画，使其突破了与空间维数的关系。也就是说超平面的VC维不会随着空间维数的变大而增加。这里似乎与**定理3.6**的结论矛盾：$R^d$ 中的 $sign(wx+d)$ 维数至少为 $d+1$ 维，而通过高斯核映射后，维数 $d$ 扩至无穷维。按照定理3.6的结论，那假设空间的维数也是无穷维。

但是**定理3.8**会认为说，由于SVM的核函数空间映射转化，**定理3.6**的结论将会没有意义——无穷维的VC维度当然是无意义的。因此，**定理3.8**利用了对于 $w$ 的约束，从而对于超平面进行了限制。在限制了超平面后，其VC维数就会降低（模型可以表达的复杂度降低）。而降低后的维数是不能够直接求解得出的，而只能够得到其VC维的上界。

此外，使用**Novikoff定理**证明感知机的收敛性，可得到相同的不等式。不同的是，**Novikoff定理**证明感知机的收敛性时，不等式的左侧为感知机算法在数据集上的误分类次数。



## 7.【证明补充】多层神经网络的复杂性的描述思路

描述多层神经网络的复杂度，需要通过两个步骤来完成。

首先，需要描述单个映射函数的增长函数，即**式3.51**的内容；其次，需要考察多层映射下的增长函数，即**引理3.2**，**引理3.3**，及**定理3.9**的内容。

其中，**引理3.2** 构造了函数族的增长函数，与其笛卡尔积的增长函数的关系；**引理3.3** 在**引理3.2**的基础之上，扩展到复合函数族的情况；最终**定理3.9**再一般化到多层神经网络的情况。



## 部分参考文献

[1] Understanding Machine Learning：3

[2] Foundations of Machine Learning：3，4

[3] Statistical Learning Theory：9.2