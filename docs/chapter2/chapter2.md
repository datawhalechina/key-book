# 第二章：可学性

*Edit: 李一飞，王茂霖，Hao ZHAN*

*Update: 06/12/2020*

---

本章的内容围绕学习理论中的可学性理论展开，主要讨论「事件否能够通过机器学习来解决」这一问题。通过学习理论事先辨别某个问题是否能够被学习，将节省大量的时间与资源。

## 1.【证明补充】经验误差的期望等于其泛化误差

**P25**提到，当样本从样本空间独立同分布采样得到时，经验误差的期望等于其泛化误差。在此，对该证明进行补充。

首先需要补充说明「经验误差」和「泛化误差」的概念：

* 泛化误差：泛化误差其实是一个理想化的误差概念。计算泛化误差需要知道样本的真实分布，而在大多数时候，数据样本的真实分布情况并不为人所知。由于人们所获得的信息大多由**采样（sampling）**后的样本提供，因此在这种缺乏真实分布信息情况下，无法求得**泛化误差**。而正是因为泛化误差的不可求，才迫使人们去寻找一种替代的方法来定义学习算法的误差。这一替代品就是**经验误差**。
* 经验误差：经验误差是指学习算法在样本上的误差。当数据与映射关系确定时，便能够求得具体的经验误差。

通过大数定理可以进一步讨论经验误差与泛化误差的关系。当样本量很大时，数据的采样分布接近于真实分布，经验误差的极限也就会趋于泛化误差。另一方面，若将每一个采样的样本都视为随机变量，那么经验误差的期望也就等于泛化误差：

$$
\mathrm{E}[\widehat{E}(h ; D)]=E(h ; \mathcal{D})
$$


证明过程分为两步，首先考察等式右边，泛化误差可表示为：
$$
E(h ; \mathcal{D})=P_{(x, y) \sim \mathcal{D}}(h(x) \neq y)=\mathbb{E}_{(x, y) \sim \mathcal{D}}[\mathbb{I}(h(x) \neq y)] \\
$$

然后考察等式左边，经验误差可表示为：
$$
\widehat{E}(h ; D)=\frac{1}{m} \sum_{i=1}^{m} \mathbb{I}\left(h\left(\boldsymbol{x}_{i}\right) \neq y_{i}\right)\\
$$

经验误差的期望为：
$$
\underset{D \sim D^{m}}{\mathrm{E}}[\widehat{R}(h)]=\frac{1}{m} \sum_{i=1}^{m} \underset{D \sim D^{m}}{\mathrm{E}}[1_{h(x_{i}) \neq y_{i}}]
$$
由于样本是服从独立同分布的，所以所有样本的期望值相同，期望的平均就等于样本的期望，因此：

$$
\underset{D \sim D^{m}}{\mathrm{E}}[\widehat{R}(h)]=\frac{1}{m} \sum_{i=1}^{m} \operatorname{E}_{D \sim D^{m}}[1_{h(x) \neq y}] = \frac{1}{m} m \operatorname{E}_{D \sim D^{m}}[1_{h(x) \neq y}]
$$

$$
\Rightarrow \underset{D \sim D^{m}}{\mathrm{E}}[\widehat{R}(h)]=\underset{D \sim D^{m}}{\mathrm{E}}[1_{(h(x)\} \in(x)\}}]=\underset{(x,y) \sim D}{\mathrm{E}}[1_{\{h(x) \neq y\}}]=\mathbb{E}_{(x, y) \sim \mathcal{D}}[\mathbb{I}(h(x) \neq y)]=E(h ; \mathcal{D})
$$

等式左右相等，证毕。

## 2.【概念补充】假设空间的可分与不可分

**P26**提到了可分性的问题，在此对这个概念进行补充说明。

* **假设空间**
  可分性是一个针对于假设空间的概念，因此需要先理清什么是假设空间。简单来说，假设空间是对于学习算法最大的能力整体刻画。假设空间表述了给定算法所有可能的映射函数。而待学习的概念是样本空间对于标记空间的映射（也就是一个函数映射）。如果目标概念存在于假设空间中，那么对于给定的学习算法，其具备某个映射函数能够正确的区分所有的样本类别。

* **可分性**
  可分性（separable）和不可分性（non-separable）是对于假设空间的一种性质的定义。即考察对于给定学习算法，是否存在潜在的、能够完全区分的映射。若存在，则该学习算法对于此假设空间可分；若不存在，则不可分。

  此处的关键在于**「潜在」**一词，是一种对于存在性的考察。假设空间中的映射并不惟一，甚至可能是无穷的。例如，在二分类的感知机的案例中，对于 $y=sign(wx+b)$ 而言，任意 $w,b$ 都可以构成一个映射。这些**「潜在」**映射就构成了感知机学习算法的假设空间。当人们说该学习算法对于假设空间可分时，意味着对于所有线性可分的样本来说，感知机的假设空间是可区分的。即存在某个 $y=wx+b$ 能够将样本区分开来。

* **严格性与能力上限**

  可分性具有一定的局限性，主要体现在严格性和能力上限的方面。

  可分性的严格性是指，其需要对于所有的样本都可分。有时，由于噪声或者异常值的影响，数据并非完全可区分的，算法只能区分绝大多数的样本。因此可分性没有完全地定义了学习算法的有效性。

  其次，可分性仅仅表示了学习算法的能力上限。例如，当我们在线性模型中使用高斯核技巧时，能够对任意二分类样本进行区分（维度为无穷）。但从这样一个庞大的假设空间中找到正确对应的映射函数却十分困难。这在深度学习中表现的最为明显。在这个意义上，可分性仅仅表示了学习算法能力上限而已。



## 3.【概念补充】关于$size(c)$

**P27**中的**定义2.2**，**定义2.3**和**定义2.4**通过多项式函数 $poly(1/\epsilon,1/\delta,size(x),size(c))$ 定义了PAC可学、PAC学习算法和样本复杂度等概念。但是对于  $size(c)$ 并没有过多描述，故在此对其进行补充说明。

- **概念的表示**

当谈论一个概念或者概念类的时候，通常使用了一些固有的**「表示方案」**来描述该概念。所谓「表示方案」，即可以通过函数 $\mathcal{R} : \Sigma^* \rightarrow \mathcal{C}$  来实现对概念的表征的方法。其中 $\Sigma$ 是有限符号表（例如{0, 1}），$\Sigma^*$ 是符号组成的合法字符串集合。对于 $\Sigma^*$ 中的任意一个字符串 $\sigma$ ，通过映射 $\mathcal{R}(\sigma) = c$ 产生了概念 c 在 $\mathcal{R}$ 下的一个表示。当然，某概念在 $\mathcal{R}$ 下可能有多种表示。

- **表示的复杂度**

既然概念可以有不同的表示，那么自然也产生了对表示复杂度的讨论。假设与 $\mathcal{R}$ 相关联的映射 $size:\Sigma^* \rightarrow \mathcal{C}$ 可以为每个表示形式分配一个自然数 $size(h)$。  其中， $size(\cdot)$ 是任何符合自然定义的映射关系。例如在 $\Sigma = \{0，1\}$ 的情形中，我们将 $size(h)$ 定义为了 $h$ 的二进制长度。当二进制表示不方便时，亦可采用其他的表示方式——但无论如何，新的定义方式始终能够被一个关于二进制长度多项式所描述。例如，我们可以将决策树的大小定义为树中的节点数，无论我们使用什么样的方法对该决策树进行编码，$size(h)$始终可以用一个关于二进制字符串的长度的多项式来进行描述。

- **概念的复杂度**

概念的表示越复杂，越有利于表征概念本身；概念的表示越简单，越不利于表征概念本身。由于在评判目标概念的复杂度时，需要考虑到最差的情况，因此人们往往将目光集中于最简单的概念表示上（此时最不利于了解概念）。由此，定义概念的复杂度 $size(c)$ 为所有表示方案中最简单的表示所对应的复杂度，即：
$$
size(c) = min_{\mathcal{R}(\sigma)=c}\{size(\sigma)\}
$$

## 4.【概念补充】时间复杂度

**P27**介绍了时间复杂度和样本复杂度的概念，在此对二者的等价性进行补充。

由于不同的机器、操作系统都会带来完全不一样的运行时间，因此在考察时间复杂度时通常会使用抽象机。抽象机通常是抽象意义上的图灵机或实体意义上的图灵机。在该抽象机中，时间复杂度被定义为了「需要执行的“操作”数量」（关于时间复杂度的严谨表述，见附录）。

由于不同的机器、操作系统都会带来完全不一样的运行时间，因此在考察时间复杂度时通常会使用抽象机。抽象机通常是抽象意义上的图灵机或实体意义上的图灵机。在该抽象机中，时间复杂度被定义为了「需要执行的“操作”数量」。[^1]

一般学习问题是否可以有效解决的问题，取决于如何将其分解为一系列特定的学习问题。考虑学习有限假设类的问题，例如训练示例的数量为 $m_H(\epsilon，δ)= log(|\mathcal{H}|/δ)/\epsilon^2$ 的数量级的情况。如果，对一个 $h$ 评估会花费固定的时间，那么久可以通过对 $\mathcal{H}$ 进行详尽搜索，在时间 $O(|H|m_H(\epsilon，δ))$ 内完成这项任务。对于任何固定的有限假设类 $H$，穷举搜索算法都可以在在多项式时间内运行。如果问题序列 $|H_n| = n$ ，那么穷举搜索被认为是高效的；如果 $|H_n| = 2 n$ ，则样本复杂度是 $n$ 的多项式，而穷举搜索算法的计算复杂度随 $n$ 呈指数增长。此时，穷举搜索被认为是低效的。



## 5.【概念补充】样本复杂度

**P27**提到了样本复杂度（Sample Complexity）的概念，在此进行补充说明。

- **样本复杂度定义**

**样本复杂度**由这样一个二元函数决定： $ \mathcal{m}_{\mathcal{H}}:(0,\;1)^2\rightarrow\mathbb{N}$ ，其中，二元函数 $\mathcal{m}$ 的两个自变量分别是要求的精度 (accuracy) $\epsilon$ 和要求的置信度 (confidence) $\delta$ 。同样， $\mathcal{m}$ 也与假设空间$\mathcal{H}$ 有关系.

每个有限假设类都是PAC可学习且拥有样本复杂度：
$$
\mathcal{m}_{\mathcal{H}}(\epsilon,\delta)\leq[\frac{log(|H|/\delta)}{\epsilon}]
$$

- **样本复杂度与数据复杂度$size(c)$区分**

注意，这里的样本复杂度与之前的数据复杂度 $size(c)$ 不同，后者指的是一个数据本身的复杂程度，而这里的样本复杂度是关于假设空间$\mathcal{H}$需要样本数量的描述，也即对一个算法，我们需要多少样本才能获得一个 PAC 学习的解  （probably approximately correct solution) 。

## 6.【证明补充】布尔合取式

**P30**中提到了布尔合取式的PAC可学问题，在此对该证明进行补充。

布尔合取式是对所有布尔值的合取计算。例如，一个合取 $C_1$ 可以是 ${x}_1 \land {x}_2 \land \bar{x}_4$ 。对于这样的一个概念来说，所有满足（0，1，？，1）的样本都是正样本，而不满足（0，1，？，1）的样本均为负样本。当我们拥有足够的样本及其对应的标签时，我们就可以学习到类似 $C_1$ 这样的概念了。

我们用一个更简单的案例来说明如何从样本中学习类似这样的概念。考虑一个新的概念 $C_2 = x_1 \land \bar{x}_3 $ ，此时我们有数据

|  X1  |  X2  |  X3  |  y   |
| :--: | :--: | :--: | :--: |
|  1   |  1   |  1   |  0   |
|  1   |  0   |  0   |  1   |
|  0   |  0   |  1   |  0   |
|  1   |  1   |  0   |  1   |
|  +   |  ？  |  -   |      |

对于所有的 $y=1$，$x_1$都为1，因此在概念$C_2$ 中 $x_1$ 极有可能是+的；对于所有的 $y=1$，$x_3$都为0，因此在概念$C_2$ 中 $x_1$ 极有可能是-的；对于$x_2$，对应的y可能为0，也可能为1，因此判断 $x_2$ 与概念$C_2$ 无关。由此，得到  $C_2 = x_1 \land \bar{x}_3 $ 。

而在这里，[Kearns and Vazirani, 1994] 他们所要证明的就是 $C_n$ 是PAC可学的。



## 7.【案例补充】可知学习

**P31**提到了可知学习与不可知学习，这里用一个案例来进行说明，

在布尔函数中，关于 $d$ 个输入变量有 $2^d$ 种可能。也就是说，当输入的样本量为 $d$ 时，最多有 $2^d$ 种情况。例如，当有2个输入时，一共有4种可能的情况出现，这4种情况对应于16种二元函数假设，如下表所示。

| x1   | x2   | h1   | h2   | h3   | h4   | h5   | h6   | h7   | h8   | h9   | h10  | h11  | h12  | h13  | h14  | h15  | h16  |
| ---- | ---- | :--- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 1    | 1    | 1    | 1    | 1    | 1    | 1    |
| 0    | 1    | 0    | 0    | 0    | 0    | 1    | 1    | 1    | 1    | 0    | 0    | 0    | 0    | 1    | 1    | 1    | 1    |
| 1    | 0    | 0    | 0    | 1    | 1    | 0    | 0    | 1    | 1    | 0    | 0    | 1    | 1    | 0    | 0    | 1    | 1    |
| 1    | 1    | 0    | 1    | 0    | 1    | 0    | 1    | 0    | 1    | 0    | 1    | 0    | 1    | 0    | 1    | 0    | 1    |

每个不同的训练实例都会去掉一半的假设。例如，假设我们有 $x_1 = 0 , x_2 = 1$ ，输出为 0 ；这就删除了 $h5、h6、h7、h8、h13、h14、h15、h16$ 。这就是学习的一种方式。在布尔函数的情况下，为了得到一个单一的假说，我们需要看到所有的二维训练实例。如果我们得到的训练集只包含所有可能的实例中的一小部分子集——也就是说，如果我们只知道只有一小部分实例的输出应该是什么，那么解就不是唯一的。在看到N个例子之后，还有 $2^{2^d-N}$ 个可能的函数。



## 部分参考文献


[1] Understanding Machine Learning：3.1，8.1

[2] Foundations of Machine Learning：2

[3] An Introduction to Computational Learning Theory：1.2



---

## 附录：时间复杂度严谨描述


可以分两个步骤定义学习的复杂性。考虑固定学习问题的计算复杂性（由三元组 $(Z, H, \mathcal{l})$ ——学习数据集 (domain set) ，基准假设类 (benchmark hypothesis class) 和损失函数确定）。然后，在第二步中，我们考虑一系列此类任务的复杂度变化情况。

1. 给定一个函数$f：（0,1)^2→\mathbb{N}$ ，一个任务 $(Z, H, \mathcal{l})$ 和一个学习
   算法 $\mathcal{A}$ ，我们说 $\mathcal{A}$ 能在 $O(f)$ 时间内解决学习任务，如果存在某个常数 c ，对于每个 Z 上的概率分布 D ，算法 $\mathcal{A}$ 基于从分布 D 中独立同分布  (i.i.d) 采样得到的样本，给定的 $\epsilon,\;\delta \in (0,1) $ ，能够满足以下条件：
   - A最多执行 $cf(\epsilon, \delta)$ 个运算后终止
   - 表示为 $h_A$ 的 $\mathcal{A}$ 的输出在最多使用 $cf(\epsilon, \delta)$ 个示例之后即可用于预测新标签,。
   - A的输出是 PAC (probably approximately correct) 的，即不超过 $\delta$ 的错误率使结果误差不超过 $\epsilon$ 

2. 考虑一系列学习问题， $(Z_n, H_n, \mathcal{l_n})_{n=1}^∞$ ，其中问题 n 由学习数据集 $Z_n$ ，假设类 $H_n$ 和损失函数 $\mathcal{l}_n$ 定义。设 $\mathcal{A}$ 为设计用于解决学习问题的学习算法这种形式。给定一个函数 $g: \mathbb{N} × (0,1)^2 \rightarrow  \mathbb{N}$, 则 $\mathcal{A}$ 的耗时是O（g）。如果对于所有 n ，A求解问题 $(Z_n, H_n, \mathcal{l_n})_{n=1}^∞$ 用时 $O(f_n)$ ，其中$f_n：(0,1)^2\rightarrow\mathbb{N}$ 定义为 $f_n（\sigma，δ）＝ g（n，\sigma，δ）$。